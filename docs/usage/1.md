# 1.0 Prerequisites & Setup

The OpenShift Migration Operator (mig-operator) assists with installation of app migration tooling on OpenShift 3.x and 4.x clusters:

- Migration Controller (mig-controller)
- Migration UI (mig-ui)
- Velero

The above suite of tools is collectively referred to as _CAM_ (cluster app migration) in some parts of this guide.

Before using mig-operator to attempt a migration, verify that the OpenShift clusters you'll be migrating apps between meet version requirements:

| Product         | Versions    |
| -----------     | ----------- |
| OpenShift 3.x   | v3.7+       |
| OpenShift 4.x   | v4.1+       |

Note that temporary object storage will be required perform a migration:
- [AWS S3](https://aws.amazon.com/s3/)
- [NooBaa](https://www.noobaa.io/)
- [Minio](https://min.io/)

## 1.1 Base requirements

* A computer with access to the Internet :-)
* SSH client (for Microsoft Windows users [Putty](https://www.putty.org/) is recommended)
* Firefox or Chromium / Chrome
* oc client [Download from try.openshift.com](http://try.openshift.com)



## 1.2 Setup with mig-operator

On both the _source_ and the _destination_ cluster, we'll use mig-operator to install components needed to perform a migration.

```
# Login to your source cluster (apps will be migrated FROM here)
oc login https://my-source-cluster:8443

# Create mig-operator (requires cluster-admin privilege)
oc create -f https://raw.githubusercontent.com/fusor/mig-operator/master/operator.yml

--------------------------------------------------------------------------------------

# Login to your destination cluster (apps will be migrated TO here)
oc login https://my-destination-cluster:8443

# Create mig-operator (requires cluster-admin privilege)
oc create -f https://raw.githubusercontent.com/fusor/mig-operator/master/operator.yml
```

Next up, we'll tell mig-operator which components it should install for us by creating a `MigrationController` resource.

## 1.3 Creating a `MigrationController` resource

When you install mig-operator, it introduces a `MigrationController` CRD to your cluster.

The sample [`controller-3.yml`](https://raw.githubusercontent.com/fusor/mig-operator/master/controller-3.yml) and [`controller-4.yml`](https://raw.githubusercontent.com/fusor/mig-operator/master/controller-4.yml) are provided with recommended defaults for:
- Using OpenShift 3.x as the source cluster (_only_ Velero installed)
- Using OpenShift 4.x as the destination _cluster_ (_all_ components: Velero, mig-controller, and mig-ui installed)

If you wish to run app migrations with a different configuration, it's as simple as changing values within the `spec` of each sample MigrationController yaml definition. Note that it is _critical_ that the [mig-controller](https://github.com/fusor/mig-controller) component should only be running on _one of the two_ clusters involved with a migration.

### OpenShift 3.x 
For each OpenShift 3.x cluster that will be involved in the migration, use [`controller-3.yml`](https://raw.githubusercontent.com/fusor/mig-operator/master/controller-3.yml) as a starting point, making modifications as needed.


### OpenShift 4.x
For each OpenShift 4.x cluster that will be involved in the migration, use [`controller-4.yml`](https://raw.githubusercontent.com/fusor/mig-operator/master/controller-4.yml) as a starting point, making modifications as needed.

## 1.3.1 Configuring the MigrationController spec

The snippet below shows the contents of controller-3.yml, a starting point for installing app migration components on OpenShift 3.x.

```
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
spec:
  [...]
  # 'migration_velero' should always be 'true'
  migration_velero: true 

  # 'migration_controller' and 'migration_ui' should be 'true' only on the destination cluster
  migration_controller: false
  migration_ui: false

  [...]

  # To install the controller on OpenShift 3 you will need to configure the API endpoint:
  # mig_ui_cluster_api_endpoint: https://replace-with-openshift-cluster-hostname:8443
```

Sample 'source' cluster MigrationController CR config:
```
  [...]
  migration_velero: true 
  migration_controller: false
  migration_ui: false
  [...]
```

Sample 'destination' cluster MigrationController CR config
```
  [...]
  migration_velero: true 
  migration_controller: true
  migration_ui: true
  [...]
```

You may have also noticed the commented out option for specifying an API endpoint. 

```
  [...]
  # To install the controller on OpenShift 3 you will need to configure the API endpoint:
  # mig_ui_cluster_api_endpoint: https://replace-with-openshift-cluster-hostname:8443
  [...]
```
This should be set to the API endpoint of the OpenShift cluster where the UI will run.

## 1.3.2 Creating the MigrationController

After making necessary edits to the provided starter MigrationController yaml, login to each cluster and create the MigrationController resource. This will kick off creation of the CAM componenets.

```
# Source Cluster
oc login https://my-source-cluster:8443
oc create -f migrationcontroller-src.yml

# Destination Cluster
oc login https://my-destination-cluster:8443
oc create -f migrationcontroller-dest.yml
```

### 1.3.3 Monitoring the work of mig-operator

Once you have created the MigrationController CR, mig-operator will start working on your request. After a few minutes have passed, the complete results should be visible by looking at the running Pods in the `openshift-migration` namespace.

```
# Resulting Pods if you installed all components (destination cluster)

$ oc get pods -n openshift-migration
NAME                                  READY     STATUS    RESTARTS   AGE
controller-manager-89b4f85bb-gtrb8    1/1       Running   0          8m
migration-operator-69546687dd-8trfq   2/2       Running   0          8m
migration-ui-65cc6c6f9f-vmqf8         1/1       Running   0          8m
restic-mh5lb                          1/1       Running   0          7m
restic-p849t                          1/1       Running   0          7m
velero-84b8d9878b-gklfb               1/1       Running   0          8m
```

```
# Resulting Pods if you installed only Velero (source cluster)

$ oc get pods -n openshift-migration
NAME                                  READY     STATUS    RESTARTS   AGE
migration-operator-69546687dd-8trfq   2/2       Running   0          8m
restic-mh5lb                          1/1       Running   0          7m
restic-p849t                          1/1       Running   0          7m
velero-84b8d9878b-gklfb               1/1       Running   0          8m
```


## 1.4 CORS Configuration

After following the steps above, you should have installed the migration UI on _one of your clusters_.

CAMs UI is served out of the cluster
behind its own route, there are 3 distinct origins at play:

* The UI - (Ex: https://mig-ui-mig.apps.examplecluster.com)
* The OAuth Server - (Ex: https://openshift-authentication-openshift-authentication.apps.examplecluster.com)
* The API Server - (Ex: https://api.examplecluster.com:6443)

When the UI is served to the browser through it's route, the browser recognizes
its origin, and **blocks AJAX requests to alternative origins**. This is called
[Cross-Origin Resource Sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS),
and it's a deliberate browser security measure.

The full description of CORS is linked above, but without configuring the non-UI
origin servers, the requests will be blocked. To enable these requests,
the UI's origin should be whitelisted in a `corsAllowedOrigins` list for each
alternative origin. The servers should recognize this list and inspect the
origin of incoming requests. If the origin matches one of the CORS whitelisted  
origins (the UI), there are a set of headers that are returned in the response
that inform the browser the CORS request is accepted and valid.

Additionally, for the same reasons described above, any requests that the UI
may make to source 3.x clusters will also have to be whitelisted by configuring
the same field in the 3.x cluster master-config.yaml. This causes the 3.x API
servers to accept the CORS requests incoming from the UI that was served out
of its OCP4 cluster's route.

### 1.4.1 Configuring CORS on OpenShift 3.x
In order to enable the UI to talk to an OpenShift 3 cluster (whether local or remote) it is necessary to edit the master-config.yaml and restart the OpenShift master nodes. 

To determine the CORS URL that needs to be added retrieve the route URL after installing the controller.
```
oc get -n openshift-migration route/migration -o go-template='{{ .spec.host }}{{ println }}'
```

SSH into the OpenShift master nodes, noting the hostname from above

Add the hostname to `/etc/origin/master/master-config.yaml` under corsAllowedOrigins:
```
corsAllowedOrigins:
- //$output-from-previous-command
```

### 1.4.2 Configuring CORS on OpenShift 4.x
On OpenShift 4 cluster resources are modified by the operator if the controller is installed there and you can skip these steps. If you chose not to install the controller on your OpenShift 4 cluster you will need to perform these steps manually.

If you haven't already, determine the CORS URL that needs to be added retrieve the route URL
```
oc get -n openshift-migration route/migration -o go-template='{{ .spec.host }}{{ println }}'
```

`oc edit authentication.operator cluster` and ensure the following exist:
```
spec:
  unsupportedConfigOverrides:
    corsAllowedOrigins:
    - //localhost(:|$)
    - //127.0.0.1(:|$)
    - //$output-from-previous-command
```

`oc edit kubeapiserver.operator cluster` and ensure the following exist:
```
spec:
  unsupportedConfigOverrides:
    corsAllowedOrigins:
    - //$output-from-previous-command
```

## 1.3 Prepare to use mig-ui in your Browser
To visit the UI, look at the route on the cluster where you specified mig-ui should be installed.
```bash
$ oc get routes migration -n mig -o jsonpath='{.spec.host}'
migration-mig.apps.samplecluster.com
```

Looking at the sample output above, we'd visit the URL below from our browser:
  * https://migration-mig.apps.samplecluster.com

### 1.3.1 Accept Certificates on Source and Destination Clusters

1. Before you can login you will need to accept the certificates with your
   browser on both the source and destination cluster. To do this:
  * Visit the link displayed by the webui for `.well-known/oauth-authorization-server`.
    * For example:
      * OCP 4.1: https://api.samplecluster.com:6443/.well-known/oauth-authorization-server
      * OCP 3.11: https://master1.samplecluster.com/.well-known/oauth-authorization-server
      * OCP 3.11: https://master1.samplecluster.com/api/v1/namespaces
  * Refresh the page
  * Get redirected to login page
2. Login with your credentials for the cluster.

## 1.4 Object Storage Setup

CAM componenets use S3 object storage as temporary scratch space when performing migrations.  This storage can be any object storage that presents an `S3 like` interface.  Currently, we have tested AWS S3, Noobaa, and Minio.  

### 1.4.1 Object Storage Setup with NooBaa

NooBaa can run on an OpenShift cluster to provide an S3 compatible store for migration scratch space.

1. Download the noobaa v1.1.0 CLI from https://github.com/noobaa-operator/releases. 
2. Ensure you have available PVs with capacities of 10 Gi, 50Gi. NooBaa will create PVCs to consume these PVs.
```
# NooBaa PV usage requirements
NAME                          CAPACITY   ACCESS MODES                           
logdir-noobaa-core-0          10Gi       RWO                                    
mongo-datadir-noobaa-core-0   50Gi       RWO,RWX
```

3. Install NooBaa, take note of the output values for 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY' for a later step. Also note that an initial bucket has been created called 'first.bucket'
```
$ noobaa install --namespace noobaa

[...]
INFO[0002] System Status:                               
INFO[0003] ✅ Exists: NooBaa "noobaa"                    
INFO[0003] ✅ System Phase is "Ready"                    
[...]
INFO[0003] AWS_ACCESS_KEY_ID: ygeJ5GzAwbBJiSukw8Lv      
INFO[0003] AWS_SECRET_ACCESS_KEY: so2C5X/ttRhiX00DZrOnv0MxV0r5VlOkYmptTU91 
```

4. Expose the NooBaa S3 service to hosts outside the cluster.
```
oc expose svc s3 -n noobaa
```

5. Take note of the NooBaa S3 route URL for a later step.
```
oc get route s3 -n noobaa -o jsonpath='http://{.spec.host}'
```


Next Lab: [Lab 3 - CPMA Overview](./2.md)<br>
[Home](./README.md)
